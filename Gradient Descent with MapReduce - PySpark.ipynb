{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Daniel Acuna <deacuna@syr.edu>\n",
    "\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers either from your classmates or from the internet__\n",
    "- You can put the homework files anywhere you want in your http://notebook.acuna.io workspace but _do not change_ the file names. The TAs and the professor use these names to grade your homework.\n",
    "- Remove or comment out code that contains `raise NotImplementedError`. This is mainly to make the `assert` statement fail if nothing is submitted.\n",
    "- The tests shown in some cells (i.e., `assert` and `np.testing.` statements) are used to grade your answers. **However, the professor and TAs will use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before downloading and submitting your work through Blackboard, remember to save and press `Validate` (or go to \n",
    "`Kernel`$\\rightarrow$`Restart and Run All`). \n",
    "- Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code creates the spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Map-Reduce: Gradient descent\n",
    "\n",
    "Throughout this assignment, you should use vanilla Python and not Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistical models $f(x)$ are learned by optimizing a loss function $L(\\Theta)$ that depends on a set of parameters $\\Theta$. There are several ways of finding the optimal $\\Theta$ for the loss function, one of which is to iteratively update following the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla L = \n",
    "\\begin{pmatrix} \n",
    "    \\frac{\\partial L}{\\partial \\theta_0}\\\\ \n",
    "    \\frac{\\partial L}{\\partial \\theta_1} \\\\ \n",
    "    \\vdots\\\\ \n",
    "    \\frac{\\partial L}{\\partial \\theta_p}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "To then, compute the update\n",
    "$$\\Theta^{t+1} = \\Theta^t - \\eta \\nabla L$$\n",
    "\n",
    "Because we assume independence between data points, the gradient becomes a summation:\n",
    "\n",
    "$$\\nabla L = \\sum_{i=1}^{n} \\nabla L_i$$\n",
    "where $L_i$ is the loss function for the $i$-th data point.\n",
    "\n",
    "Take as example, the statistical model $f(x) = b_0 + b_1 x$ and loss function $L(\\Theta) = (f(x) - y)^2$. If we have a set of three datapoints $D=\\{ (x=1,y=2), (x=-2, y=-1), (x=4, y = 3)\\}$\n",
    "\n",
    "Then the loss function for each of them is\n",
    "$L_1 = \\left(b_{0} + b_{1} - 2\\right)^{2}$, \n",
    "$L_2 = \\left(b_{0} - 2 b_{1} + 1\\right)^{2}$, and\n",
    "$L_3 = \\left(b_{0} + 4 b_{1} - 3\\right)^{2}$\n",
    "\n",
    "with \n",
    "$$\\nabla L_i = \\left[\\begin{matrix}2 b_{0} + 2 b_{1} x_i - 2 y_i\\\\2 x_i \\left(b_{0} + b_{1} x_i - y_i\\right)\\end{matrix}\\right]$$\n",
    "\n",
    "if we start with a solution $b_0 = 0, b_1 = 1$, then the gradients are:\n",
    "\n",
    "$$\\nabla L_1 = \\left[\\begin{matrix}-2\\\\-2\\end{matrix}\\right]$$\n",
    "$$\\nabla L_2 = \\left[\\begin{matrix}-2\\\\4\\end{matrix}\\right]$$\n",
    "$$\\nabla L_3 = \\left[\\begin{matrix}2\\\\8\\end{matrix}\\right]$$\n",
    "\n",
    "which after accumulation would yield\n",
    "$$\\nabla L = \\left[\\begin{matrix}-2\\\\10\\end{matrix}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (5 pts)\n",
    "\n",
    "Create a function `f_linear(b, x)` that receives the parameters `b` and one data point `x` as lists and return the prediction for that data point. Assume that the first element of `b` is the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1ee003cc0679b4304ff2fa404558cd6f",
     "grade": false,
     "grade_id": "cell-e260b0944cc2cafb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define below the function `f_linear` which performs a linear prediction based on parameters as data point\n",
    "def f_linear(b,x):\n",
    "    y_hat=b[0]\n",
    "    for i in range(len(x)):\n",
    "        y_hat=y_hat+x[i]*b[i+1]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above, if we assume b = [0, 1], and the first data point x = [1], y = 2\n",
    "f_linear([0, 1], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4cdbdeeae72fba84e40d267674140347",
     "grade": true,
     "grade_id": "cell-e12a1ead7bf3835d",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# test (5 pts)\n",
    "assert f_linear([1, 1, 2, 3], [2, 1, 3]) == 14\n",
    "assert f_linear([1], []) == 1\n",
    "assert f_linear([0, 1, 0, 1, 0, 1], [0, 10, 10, 10 , 10]) == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (5 pts)\n",
    "Define the function `L(y_pred, y)` that receives a prediction `y_pred` and the actual value `y` and returns the squared error between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7892a587f9bd66149dc1402292150369",
     "grade": false,
     "grade_id": "cell-b624ab75860c6e6e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def L(y_pred, y):\n",
    "    error = (y_pred-y)*(y_pred-y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there should be not error here\n",
    "L(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2^2 error\n",
    "L(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7857ffedf06e01569d1fa76de2392278",
     "grade": true,
     "grade_id": "cell-335fe2ab491bce32",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "assert L(1, 1) == 0\n",
    "assert L(0, 4) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (10 pts)\n",
    "Create a function `gf_linear(f, b, x, y)` which returns the gradient of the linear function `f` with parameter `b` with respect to the squared loss function, evaluated at `x` and the actual outcome `y`. This function should return a vector with each element $j$ corresponding to the gradient with respect $b_j$, with $j = \\{0, 1, \\dots, p\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f7329f34aa05e0b8ef30faedf9750a13",
     "grade": false,
     "grade_id": "cell-2d3493c9766da5c1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define `gf_linear`\n",
    "def gf_linear(f, b, x, y):\n",
    "    f = f_linear(b,x)\n",
    "    gradient = []\n",
    "    if len(x)==0:\n",
    "        gradient.append(2*(f-y))\n",
    "        return gradient\n",
    "    else:\n",
    "        gradient.append(2*(f-y))\n",
    "        #y_hat = [x[i]*b[i+1] for i in range(len(x))]\n",
    "        #y_hat1 = y_hat + [b[0]]\n",
    "        \n",
    "        #grad = [2*x[i]*(f-y) for i in range(len(x))]\n",
    "        #gradient.append(grad)\n",
    "        for i in range(len(x)):\n",
    "            g=2*(f-y)*x[i]\n",
    "            gradient.append(g)\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, -2]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above and first data point\n",
    "x = [1]\n",
    "y = 2\n",
    "b = [0, 1]\n",
    "gf_linear(f_linear, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2, 4]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the example above and second data point\n",
    "x = [-2]\n",
    "y = -1\n",
    "b = [0, 1]\n",
    "gf_linear(f_linear, b, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcc6f859ba2c7ba8deb2a7232799d986",
     "grade": true,
     "grade_id": "cell-bc73ed6d8073de74",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## (10 pts)\n",
    "np.testing.assert_array_equal(gf_linear(f_linear, [0, 1], [1], 2), [-2, -2])\n",
    "np.testing.assert_array_equal(gf_linear(f_linear, [0, 1], [-2], -1), [-2, 4])\n",
    "np.testing.assert_array_equal(gf_linear(f_linear, [1], [], 0), [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (15 pts)\n",
    "\n",
    "Develop a map-reduce job that produces a value so that the first element of the value is the mean loss function across all the data. You might use other pieces of information as part of the value to create your computation.\n",
    "\n",
    "You will implement your map function as `map_mse(f, b, L, xy)` where `f` is the function `b` are the parameters of the function `L` is the loss function and `xy` is the data. Assume that the data will come as an RDD where each element is of the format:\n",
    "\n",
    "`[x, y]` where `x` is a list and `y` is a scalar.\n",
    "\n",
    "Since the key does not matter for this map reduce job, just put a constant of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9783bb5e9503f2ef935c7cb83c2fc43b",
     "grade": false,
     "grade_id": "cell-481c573523c2b098",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "rdd_data = sc.parallelize([\n",
    "    [[1, 2], 3],\n",
    "    [[3, 1], 4],\n",
    "    [[-1, 1.5], 0],\n",
    "    [[-9, 3], 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bb5b73b9485caf4962b2cef35a565c10",
     "grade": false,
     "grade_id": "cell-f87dee01323b2e82",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create function `map_mse` below\n",
    "def map_mse(f, b, L, xy):\n",
    "    y_hat=b[0]\n",
    "    for i in range(len(xy[0])):\n",
    "        y_hat=y_hat+xy[0][i]*b[i+1]\n",
    "    loss = (y_hat-xy[1])*(y_hat-xy[1]) \n",
    "    return [0,[loss,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should apply the map function in the following way:\n",
    "\n",
    "```python\n",
    "# for an example set of `b = [0, 0, 0]`\n",
    "rdd_data.map(lambda x: map_generator(f_linear, [0, 0, 0], L, x))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [9, 1]], [0, [16, 1]], [0, [0.0, 1]], [0, [0, 1]]]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it here\n",
    "rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dbda664de2ce3dee744024f1bf661087",
     "grade": true,
     "grade_id": "cell-449c2d0c5f77a1af",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (10 pts)\n",
    "assert rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).count() == 4\n",
    "assert rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).map(lambda x: len(x)).\\\n",
    "    distinct().\\\n",
    "    first() == 2\n",
    "\n",
    "assert rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).count() == 4\n",
    "# the first element should be a number\n",
    "assert isinstance((rdd_data.map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).first()[1][0]), \n",
    "                  (int, float, complex))\n",
    "# try with other initializations\n",
    "assert isinstance((rdd_data.map(lambda x: map_mse(f_linear, [1, 2, 3], L, x)).first()[1][0]), \n",
    "                  (int, float, complex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create a reduce job that receives two values of a previous reduce (or map) and merge them appropriately. Remember that at the end of the reduce job, the first element of the value should be the mean squared error. Create the function `reduce_mse(v1, v2)` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d4c7b7e49c6fcf2dbf3b3d525ef5e86d",
     "grade": false,
     "grade_id": "cell-664ff0abbe2fe932",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# create function `reduce_mse` below\n",
    "def reduce_mse(v1, v2):\n",
    "    return [((v1[0]*v1[1])+(v2[0]*v2[1]))/(v1[1]+v2[1]), \n",
    "            v1[1]+v2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following function call should return the mean squared error\n",
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.8125"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following function call should return the mean squared error\n",
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 2, 3], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ddfe12639adf21efa38a1554e59cdb37",
     "grade": true,
     "grade_id": "cell-39ff3270d4901c44",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "assert rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [0, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0] == 6.25\n",
    "\n",
    "assert rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0] == 3.25\n",
    "\n",
    "assert rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 2, 3], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0] == 41.8125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.\\\n",
    "    map(lambda x: map_mse(f_linear, [2, 0, 0], L, x)).\\\n",
    "    reduceByKey(reduce_mse).first()[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 pts)\n",
    "\n",
    "In this question, you will compute the cumulative gradient of a model on the data. You will define a map function `map_gradient(f, gf, b, xy)` that would receive a function `f`, its gradient `gf`, its parameters `b`, and a data point `xy = [x, y]`. Also you will define a function `reduce_gradient(v1, v2)` that combines the two values appropriately. In the map function, you probably do not need to keep extra values beyond the actual gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b5464a0f7f432697ee6d51651071c13",
     "grade": false,
     "grade_id": "cell-f6fa0087a0af6012",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the function `map_gradient` below\n",
    "def map_gradient(f, gf, b, xy):\n",
    "    g=gf(f,b,xy[0],xy[1])\n",
    "    return[0,g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2783391c2092700ae7af8efe7da67aec",
     "grade": true,
     "grade_id": "cell-85e6b947e7eab1ca",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "assert len(rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)[1]).first()) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ba1a1e364bf1c1610dba927c27ce4ca8",
     "grade": false,
     "grade_id": "cell-d3a5661b89fd6eec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# define the function `reduce_gradient` below\n",
    "def reduce_gradient(v1, v2):\n",
    "    # YOUR CODE HERE\n",
    "    rg=[]\n",
    "    for i in range(len(v1)):\n",
    "        rg.append(v1[i]+v2[i])\n",
    "    return rg\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-14.0, -30.0, -20.0]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ed8453696d68f53bc7e3d30156f7f8da",
     "grade": true,
     "grade_id": "cell-7f50da09ee10e7d1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# (5 pts)\n",
    "np.testing.assert_array_equal(\n",
    "    rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1],\n",
    "    [-14.0, -30.0, -20.0])\n",
    "\n",
    "np.testing.assert_array_equal(\n",
    "    rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, [0, 0, 0], xy)).\\\n",
    "    reduceByKey(reduce_gradient).first()[1],\n",
    "    [-14.0, -30.0, -20.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if all your answers are correct, then we can run an optimization below, and the MSE should decrease with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.14, 0.3, 0.2]\n",
      "Current MSE: \t\t 4.036099999999999\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.27480000000000004, 0.15880000000000005, 0.455]\n",
      "Current MSE: \t\t 2.8077335025\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.34362200000000004, 0.41343399999999997, 0.540541]\n",
      "Current MSE: \t\t 2.124745162297563\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.42466317000000003, 0.24800435, 0.707635855]\n",
      "Current MSE: \t\t 1.7435970621414336\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.45430526015000006, 0.47522477825, 0.730516771125]\n",
      "Current MSE: \t\t 1.529538732230305\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.50541029705925, 0.29867069991675, 0.848308677264375]\n",
      "Current MSE: \t\t 1.4080112827808984\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5135716556948637, 0.5084709260312962, 0.8371720415554381]\n",
      "Current MSE: \t\t 1.3377595648707117\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5479266281297145, 0.32798388034815074, 0.9270367149304003]\n",
      "Current MSE: \t\t 1.2959552690942715\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5443950562815554, 0.5259519919004072, 0.8977132121221939]\n",
      "Current MSE: \t\t 1.2699656129248922\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5693007089887507, 0.34545294325487574, 0.971494595933439]\n",
      "Current MSE: \t\t 1.252798997362518\n"
     ]
    }
   ],
   "source": [
    "b = [0, 0, 0]\n",
    "learning_rate = 0.01\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 pts)** In the code, above, play with the value of `learning_rate` less than 1.0 until the optimizer diverges (the loss function goes down and then goes *up*). What is this learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc837820a5a204f6d39c8fd064139e4b",
     "grade": true,
     "grade_id": "cell-49ee8798a623ecdf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \t [0, 0, 0]\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.112, 0.24, 0.16]\n",
      "Current MSE: \t\t 4.234704000000001\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.220672, 0.197632, 0.3552]\n",
      "Current MSE: \t\t 3.017263745024\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.294897664, 0.301457408, 0.470676992]\n",
      "Current MSE: \t\t 2.267273760120193\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.36048288563200004, 0.30299682815999995, 0.586261239808]\n",
      "Current MSE: \t\t 1.7999525928255933\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.40814832767795206, 0.35202608029696003, 0.6645021785702401]\n",
      "Current MSE: \t\t 1.5068607387361177\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.44808107698664246, 0.3628787486993285, 0.7351156190122804]\n",
      "Current MSE: \t\t 1.3223573040294068\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.47802637365315925, 0.3881404867532521, 0.786658238341638]\n",
      "Current MSE: \t\t 1.2059586751193274\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5022951838666727, 0.3984717197396242, 0.8307047545536558]\n",
      "Current MSE: \t\t 1.1324247820855144\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5207170066477709, 0.4124866736462725, 0.8642714629278032]\n",
      "Current MSE: \t\t 1.0859224573819415\n",
      "New iteration\n",
      "=============\n",
      "Current solution: \t [0.5352772633410193, 0.42026119273799944, 0.8921698310598404]\n",
      "Current MSE: \t\t 1.0564853707235669\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "b = [0, 0, 0]\n",
    "learning_rate = 0.009\n",
    "print(\"Initial solution: \\t\", b)\n",
    "for _ in range(10):\n",
    "    print(\"New iteration\")\n",
    "    print(\"=============\")\n",
    "    gradient = rdd_data.map(lambda xy: map_gradient(f_linear, gf_linear, b, xy)).\\\n",
    "        reduceByKey(reduce_gradient).first()[1]\n",
    "    b = [b0 - learning_rate*g0 for b0, g0 in zip(b, gradient)]\n",
    "    print(\"Current solution: \\t\", b)\n",
    "    mse = rdd_data.\\\n",
    "        map(lambda x: map_mse(f_linear, b, L, x)).\\\n",
    "        reduceByKey(reduce_mse).first()[1][0]\n",
    "    print(\"Current MSE: \\t\\t\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identified an optimum learning rate at **0.009**, and step towards any side results in an increase in the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
